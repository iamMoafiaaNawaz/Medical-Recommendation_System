{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#--------------------------V1-----------------------------------"
      ],
      "metadata": {
        "id": "5uWQCE7IPQ1J"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install transformers torch pandas numpy nltk matplotlib seaborn peft huggingface_hub detoxify fuzzywuzzy python-Levenshtein rouge_score datasets -q\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "else:\n",
        "    print(\"Drive already mounted at /content/drive\")\n",
        "\n",
        "# Import libraries\n",
        "import json\n",
        "import time\n",
        "import logging\n",
        "import gc\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.manifold import TSNE\n",
        "from transformers import AutoTokenizer, AutoModel, Trainer, TrainingArguments\n",
        "from huggingface_hub import login\n",
        "import detoxify\n",
        "from fuzzywuzzy import fuzz\n",
        "from rouge_score import rouge_scorer\n",
        "from datasets import load_metric\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import sqlite3\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configure logging\n",
        "BASE_DIR = \"/content/drive/My Drive/MedicalRecommenderV1\"\n",
        "os.makedirs(BASE_DIR, exist_ok=True)\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "file_handler = logging.FileHandler(os.path.join(BASE_DIR, \"recommender.log\"))\n",
        "file_handler.setLevel(logging.DEBUG)  # Set to DEBUG for detailed logs\n",
        "file_handler.setFormatter(logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\"))\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.addHandler(file_handler)\n",
        "\n",
        "# Configuration class\n",
        "class Config:\n",
        "    HF_TOKEN = \"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\"\n",
        "    DATA_PATH = \"/content/drive/MyDrive/Colab Notebooks/pre-processed_Data.csv\"\n",
        "    MODEL_NAME = \"dmis-lab/biobert-base-cased-v1.1-mnli\"\n",
        "    MODEL_DIR = os.path.join(BASE_DIR, \"fine_tuned_model\")\n",
        "    EMBEDDINGS_PATH = os.path.join(BASE_DIR, \"case_embeddings.npy\")\n",
        "    DB_PATH = os.path.join(BASE_DIR, \"user_profiles.db\")\n",
        "    MAX_LENGTH = 64\n",
        "    NUM_EPOCHS = 2\n",
        "    TRAIN_BATCH_SIZE = 1\n",
        "    LEARNING_RATE = 2e-5\n",
        "    TOP_K = 5\n",
        "    BATCH_SIZE = 16  # Increased for better embedding generation\n",
        "    SIMILARITY_THRESHOLD = 0.6  # Increased for better matching\n",
        "    LR_STEP_SIZE = 1\n",
        "    LR_GAMMA = 0.1\n",
        "    TOXICITY_MODEL = None\n",
        "    EMBEDDING_DIM = None\n",
        "    CONTRASTIVE_MARGIN = 1.0  # Adjusted for stable training\n",
        "\n",
        "    @staticmethod\n",
        "    def init_toxicity_model():\n",
        "        try:\n",
        "            Config.TOXICITY_MODEL = detoxify.Detoxify('original')\n",
        "            logger.info(\"Detoxify model loaded successfully.\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to load Detoxify model: {e}\")\n",
        "            with open(os.path.join(BASE_DIR, \"error_log.txt\"), \"a\") as f:\n",
        "                f.write(f\"Failed to load Detoxify model: {e}\\n\")\n",
        "\n",
        "# Initialize environment\n",
        "def init_environment():\n",
        "    nltk_data_dir = os.path.join(BASE_DIR, \"nltk_data\")\n",
        "    os.makedirs(nltk_data_dir, exist_ok=True)\n",
        "    os.makedirs(os.path.join(BASE_DIR, \"hf_cache\"), exist_ok=True)\n",
        "    nltk.data.path.append(nltk_data_dir)  # Set NLTK data path\n",
        "    try:\n",
        "        nltk.download('wordnet', download_dir=nltk_data_dir, quiet=True)\n",
        "        nltk.download('punkt', download_dir=nltk_data_dir, quiet=True)\n",
        "        nltk.download('omw-1.4', download_dir=nltk_data_dir, quiet=True)\n",
        "        logger.info(\"NLTK resources downloaded successfully.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to download NLTK resources: {e}\")\n",
        "        with open(os.path.join(BASE_DIR, \"error_log.txt\"), \"a\") as f:\n",
        "            f.write(f\"Failed to download NLTK resources: {e}\\n\")\n",
        "\n",
        "# Initialize user profile database\n",
        "def init_database():\n",
        "    with sqlite3.connect(Config.DB_PATH) as conn:\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(\"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS users (\n",
        "                user_id TEXT PRIMARY KEY,\n",
        "                name TEXT UNIQUE,\n",
        "                gender TEXT,\n",
        "                city TEXT,\n",
        "                past_symptoms TEXT,\n",
        "                ratings TEXT\n",
        "            )\n",
        "        \"\"\")\n",
        "        users = [\n",
        "            (\"1\", \"Iqra\", \"Female\", \"Kabirwala\", \"[]\", \"{}\"),\n",
        "            (\"2\", \"Moafi\", \"Female\", \"Lahore\", \"[]\", \"{}\"),\n",
        "            (\"3\", \"Sumair\", \"Male\", \"Quetta\", \"[]\", \"{}\")\n",
        "        ]\n",
        "        cursor.executemany(\"INSERT OR IGNORE INTO users VALUES (?, ?, ?, ?, ?, ?)\", users)\n",
        "        conn.commit()\n",
        "    logger.info(\"Initialized user profile database.\")\n",
        "    with open(os.path.join(BASE_DIR, \"output_log.txt\"), \"a\") as f:\n",
        "        f.write(\"Initialized user profile database.\\n\")\n",
        "\n",
        "# Load and preprocess data\n",
        "def load_and_preprocess_data():\n",
        "    if not os.path.exists(Config.DATA_PATH):\n",
        "        logger.error(f\"File {Config.DATA_PATH} not found.\")\n",
        "        raise FileNotFoundError(f\"File {Config.DATA_PATH} not found.\")\n",
        "    try:\n",
        "        df = pd.read_csv(Config.DATA_PATH)\n",
        "        if df.empty:\n",
        "            raise ValueError(\"Dataset is empty.\")\n",
        "        logger.info(f\"Loaded dataset with {len(df)} rows.\")\n",
        "        logger.info(f\"Unique diseases: {df['Processed_Disease'].nunique()}\")\n",
        "        logger.info(f\"Sample symptoms: {df['Processed_Symptoms'].unique()[:5]}\")\n",
        "        print(\"Sample data:\\n\", df[['Processed_Symptoms', 'Processed_Disease']].head())\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading file {Config.DATA_PATH}: {e}\")\n",
        "        with open(os.path.join(BASE_DIR, \"error_log.txt\"), \"a\") as f:\n",
        "            f.write(f\"Error loading file {Config.DATA_PATH}: {e}\\n\")\n",
        "        raise\n",
        "\n",
        "    required_columns = [\n",
        "        \"CommonAgeGroup\", \"Sex\", \"Severity\", \"Specialist\", \"Name\", \"Address/Details\",\n",
        "        \"City\", \"Rating\", \"Mapped_Category\", \"Processed_Symptoms\", \"Processed_Disease\",\n",
        "        \"Processed_Treatment\"\n",
        "    ]\n",
        "    if not all(col in df.columns for col in required_columns):\n",
        "        raise ValueError(f\"Missing columns: {df.columns.tolist()}\")\n",
        "    df = df[required_columns]\n",
        "    df = df.dropna(subset=[\"Processed_Symptoms\", \"Processed_Disease\", \"Processed_Treatment\"])\n",
        "    df = df.sample(frac=0.5, random_state=42)\n",
        "    df[\"Rating\"] = df[\"Rating\"].astype(float)\n",
        "    df.to_pickle(os.path.join(BASE_DIR, \"processed_data.pkl\"))\n",
        "    interaction_matrix = df.pivot_table(index=\"Processed_Symptoms\", columns=\"Name\", values=\"Rating\", fill_value=0)\n",
        "    doctor_similarity = cosine_similarity(interaction_matrix.T)\n",
        "    doctor_similarity_df = pd.DataFrame(\n",
        "        doctor_similarity, index=interaction_matrix.columns, columns=interaction_matrix.columns\n",
        "    )\n",
        "    logger.info(\"Processed dataset and created doctor similarity matrix.\")\n",
        "    return df, doctor_similarity_df, interaction_matrix\n",
        "\n",
        "# Load model and tokenizer\n",
        "def load_tokenizer_and_model():\n",
        "    login(token=Config.HF_TOKEN, add_to_git_credential=False)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    logger.info(f\"Using device: {device}\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_NAME, trust_remote_code=True)\n",
        "    try:\n",
        "        model = AutoModel.from_pretrained(Config.MODEL_NAME, trust_remote_code=True, use_safetensors=False)\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = True\n",
        "        model.to(device)\n",
        "        model.train()\n",
        "        trainable_params = [name for name, param in model.named_parameters() if param.requires_grad]\n",
        "        logger.info(f\"Trainable parameters: {len(trainable_params)}\")\n",
        "        if not trainable_params:\n",
        "            raise ValueError(\"No trainable parameters found!\")\n",
        "        sample_input = tokenizer(\"test\", return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**sample_input, output_hidden_states=True)\n",
        "            Config.EMBEDDING_DIM = outputs.hidden_states[-1].shape[-1]\n",
        "        logger.info(f\"Loaded {Config.MODEL_NAME} on {device}, EMBEDDING_DIM={Config.EMBEDDING_DIM}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Model loading failed: {e}\")\n",
        "        with open(os.path.join(BASE_DIR, \"error_log.txt\"), \"a\") as f:\n",
        "            f.write(f\"Model loading failed: {e}\\n\")\n",
        "        raise\n",
        "    return tokenizer, model, device\n",
        "\n",
        "# Custom dataset\n",
        "class MedicalDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_length):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.disease_groups = df.groupby(\"Processed_Disease\").indices\n",
        "        self.diseases = list(self.disease_groups.keys())\n",
        "        if not self.diseases:\n",
        "            logger.error(\"No diseases found in dataset.\")\n",
        "            raise ValueError(\"No diseases found in dataset.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        anchor_row = self.df.iloc[idx]\n",
        "        anchor_text = (\n",
        "            f\"Symptoms: {anchor_row['Processed_Symptoms']}. Age: {anchor_row['CommonAgeGroup']}. \"\n",
        "            f\"Sex: {anchor_row['Sex']}. Severity: {anchor_row['Severity']}.\"\n",
        "        )\n",
        "        anchor_inputs = self.tokenizer(\n",
        "            anchor_text, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        disease = anchor_row[\"Processed_Disease\"]\n",
        "        positive_indices = self.disease_groups.get(disease, [idx])\n",
        "        positive_idx = random.choice(positive_indices) if len(positive_indices) > 1 else idx\n",
        "        positive_row = self.df.iloc[positive_idx]\n",
        "        positive_text = (\n",
        "            f\"Symptoms: {positive_row['Processed_Symptoms']}. Age: {positive_row['CommonAgeGroup']}. \"\n",
        "            f\"Sex: {positive_row['Sex']}. Severity: {positive_row['Severity']}.\"\n",
        "        )\n",
        "        positive_inputs = self.tokenizer(\n",
        "            positive_text, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        negative_diseases = [d for d in self.diseases if d != disease]\n",
        "        if not negative_diseases:\n",
        "            negative_idx = idx\n",
        "            logger.warning(f\"No negative disease found for index {idx}. Using same index.\")\n",
        "        else:\n",
        "            negative_disease = random.choice(negative_diseases)\n",
        "            negative_indices = self.disease_groups[negative_disease]\n",
        "            negative_idx = random.choice(negative_indices)\n",
        "            while negative_idx == idx or negative_idx == positive_idx and len(negative_indices) > 1:\n",
        "                negative_idx = random.choice(negative_indices)\n",
        "        negative_row = self.df.iloc[negative_idx]\n",
        "        negative_text = (\n",
        "            f\"Symptoms: {negative_row['Processed_Symptoms']}. Age: {negative_row['CommonAgeGroup']}. \"\n",
        "            f\"Sex: {negative_row['Sex']}. Severity: {negative_row['Severity']}.\"\n",
        "        )\n",
        "        negative_inputs = self.tokenizer(\n",
        "            negative_text, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        input_ids = torch.cat([anchor_inputs[\"input_ids\"], positive_inputs[\"input_ids\"], negative_inputs[\"input_ids\"]], dim=0)\n",
        "        attention_mask = torch.cat([anchor_inputs[\"attention_mask\"], positive_inputs[\"attention_mask\"], negative_inputs[\"attention_mask\"]], dim=0)\n",
        "\n",
        "        logger.debug(f\"Sample {idx}: Anchor disease: {disease}, Negative disease: {negative_row['Processed_Disease']}\")\n",
        "        return {\n",
        "            \"input_ids\": input_ids.squeeze(1),\n",
        "            \"attention_mask\": attention_mask.squeeze(1)\n",
        "        }\n",
        "\n",
        "# Custom Data Collator\n",
        "class CustomDataCollator:\n",
        "    def __call__(self, batch):\n",
        "        input_ids = torch.cat([item[\"input_ids\"] for item in batch], dim=0)\n",
        "        attention_mask = torch.cat([item[\"attention_mask\"] for item in batch], dim=0)\n",
        "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
        "\n",
        "# Custom Trainer\n",
        "class CustomTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        try:\n",
        "            model.train()\n",
        "            batch_size = inputs[\"input_ids\"].shape[0] // 3\n",
        "            if batch_size == 0:\n",
        "                logger.error(\"Batch size is zero in compute_loss.\")\n",
        "                raise ValueError(\"Batch size is zero.\")\n",
        "            outputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], output_hidden_states=True)\n",
        "            embeddings = torch.mean(outputs.hidden_states[-1], dim=1)\n",
        "            embeddings_norm = F.normalize(embeddings, p=2, dim=1)\n",
        "            embeddings = embeddings.view(batch_size, 3, -1)\n",
        "            anchor_emb = embeddings[:, 0, :]\n",
        "            positive_emb = embeddings[:, 1, :]\n",
        "            negative_emb = embeddings[:, 2, :]\n",
        "            pos_sim = F.cosine_similarity(anchor_emb, positive_emb, dim=1)\n",
        "            neg_sim = F.cosine_similarity(anchor_emb, negative_emb, dim=1)\n",
        "            loss = torch.mean(torch.clamp(Config.CONTRASTIVE_MARGIN - pos_sim + neg_sim, min=0.0))\n",
        "            logger.debug(f\"Computed loss: {loss.item()}\")\n",
        "            if return_outputs:\n",
        "                return (loss, {\"loss\": loss, \"outputs\": outputs})\n",
        "            return loss\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in compute_loss: {e}\")\n",
        "            raise\n",
        "\n",
        "    def training_step(self, model, inputs, num_items_in_batch=None):\n",
        "        try:\n",
        "            loss = super().training_step(model, inputs)\n",
        "            grad_norm = sum(param.grad.norm(2).item() for param in model.parameters() if param.grad is not None)\n",
        "            logger.debug(f\"Training step completed. Gradient norm: {grad_norm:.4f}\")\n",
        "            return loss\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in training_step: {e}\")\n",
        "            raise\n",
        "\n",
        "# Fine-tune model\n",
        "def fine_tune_model(model, tokenizer, df, learning_rate, device):\n",
        "    logger.info(\"Starting training.\")\n",
        "    try:\n",
        "        train_size = int(0.8 * len(df))\n",
        "        train_df = df.iloc[:train_size]\n",
        "        val_df = df.iloc[train_size:].sample(n=min(500, len(df[train_size:])), random_state=42)\n",
        "        train_dataset = MedicalDataset(train_df, tokenizer, Config.MAX_LENGTH)\n",
        "        val_dataset = MedicalDataset(val_df, tokenizer, Config.MAX_LENGTH)\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=os.path.join(BASE_DIR, \"results\"),\n",
        "            per_device_train_batch_size=Config.TRAIN_BATCH_SIZE,\n",
        "            per_device_eval_batch_size=1,\n",
        "            num_train_epochs=Config.NUM_EPOCHS,\n",
        "            learning_rate=learning_rate,\n",
        "            eval_strategy=\"epoch\",\n",
        "            save_strategy=\"epoch\",\n",
        "            logging_steps=16,\n",
        "            save_total_limit=1,\n",
        "            report_to=\"none\",\n",
        "            gradient_accumulation_steps=4,\n",
        "            eval_accumulation_steps=1,\n",
        "            fp16=torch.cuda.is_available(),\n",
        "            logging_dir=os.path.join(BASE_DIR, \"logs\")\n",
        "        )\n",
        "        trainer = CustomTrainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=val_dataset,\n",
        "            data_collator=CustomDataCollator()\n",
        "        )\n",
        "        trainer.train()\n",
        "        logger.info(\"Training completed.\")\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        eval_results = trainer.evaluate()\n",
        "        with open(os.path.join(BASE_DIR, \"train_eval_results.json\"), \"w\") as f:\n",
        "            json.dump(eval_results, f, indent=2)\n",
        "        model.save_pretrained(Config.MODEL_DIR, safe_serialization=True)\n",
        "        tokenizer.save_pretrained(Config.MODEL_DIR)\n",
        "        logger.info(\"Model and tokenizer saved.\")\n",
        "        return trainer\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during training: {e}\")\n",
        "        with open(os.path.join(BASE_DIR, \"error_log.txt\"), \"a\") as f:\n",
        "            f.write(f\"Error during training: {e}\\n\")\n",
        "        raise\n",
        "\n",
        "# Extract semantic features\n",
        "def extract_semantic_features(text, model, tokenizer, device):\n",
        "    model.eval()\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=Config.MAX_LENGTH, truncation=True, padding=True).to(device)\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs, output_hidden_states=True)\n",
        "            features = torch.mean(outputs.hidden_states[-1], dim=1).to(torch.float32)\n",
        "            features = F.normalize(features, p=2, dim=1)\n",
        "        return features.cpu().numpy().flatten()\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error extracting features: {e}\")\n",
        "        with open(os.path.join(BASE_DIR, \"error_log.txt\"), \"a\") as f:\n",
        "            f.write(f\"Error extracting features: {e}\\n\")\n",
        "        return np.zeros(Config.EMBEDDING_DIM)\n",
        "\n",
        "# Generate embeddings\n",
        "def generate_case_embeddings(df, model, tokenizer, device):\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "    max_rows = min(2000, len(df))  # Increased for better embeddings\n",
        "    df_subset = df.iloc[:max_rows].copy()\n",
        "    df_subset.reset_index(drop=True, inplace=True)  # Ensure consistent indexing\n",
        "    for i in range(0, len(df_subset), Config.BATCH_SIZE):\n",
        "        batch_df = df_subset[i:i + Config.BATCH_SIZE]\n",
        "        batch_embeddings = []\n",
        "        for idx, row in batch_df.iterrows():\n",
        "            input_text = (\n",
        "                f\"Symptoms: {row['Processed_Symptoms']}. Age: {row['CommonAgeGroup']}. \"\n",
        "                f\"Sex: {row['Sex']}. Severity: {row['Severity']}.\"\n",
        "            )\n",
        "            embedding = extract_semantic_features(input_text, model, tokenizer, device)\n",
        "            if embedding.shape[0] != Config.EMBEDDING_DIM:\n",
        "                logger.warning(f\"Invalid embedding shape for index {idx}: {embedding.shape}\")\n",
        "                continue\n",
        "            batch_embeddings.append(embedding)\n",
        "        embeddings.extend(batch_embeddings)\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        logger.info(f\"Processed batch {i // Config.BATCH_SIZE + 1}/{len(df_subset) // Config.BATCH_SIZE + 1}\")\n",
        "    if not embeddings:\n",
        "        logger.error(\"No valid embeddings generated.\")\n",
        "        raise ValueError(\"No valid embeddings generated.\")\n",
        "    embeddings = np.array(embeddings, dtype=np.float32)\n",
        "    np.save(Config.EMBEDDINGS_PATH, embeddings)\n",
        "    logger.info(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
        "    return embeddings, df_subset\n",
        "\n",
        "# Preprocess input\n",
        "def preprocess_input(patient_data):\n",
        "    symptoms = patient_data[\"symptoms\"].lower().strip().replace(\",\", \" \")\n",
        "    history = patient_data.get(\"history\", \"\").lower().strip()\n",
        "    labs = patient_data.get(\"labs\", \"\").lower().strip()\n",
        "    return f\"Symptoms: {symptoms}. History: {history}. Labs: {labs}.\"\n",
        "\n",
        "# Compute advanced metrics\n",
        "def compute_metrics(recommendations, df, case_embeddings, model, tokenizer, device, top_k=Config.TOP_K):\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    y_scores = []\n",
        "    latencies = []\n",
        "    meteor_scorer = load_metric(\"meteor\")\n",
        "    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "\n",
        "    for name, rec in recommendations.items():\n",
        "        patient_symptoms = rec[\"symptoms\"].lower().replace(\",\", \" \")\n",
        "        actual_conditions = []\n",
        "        for _, row in df.iterrows():\n",
        "            dataset_symptoms = str(row[\"Processed_Symptoms\"]).lower()\n",
        "            score = fuzz.partial_ratio(patient_symptoms, dataset_symptoms)\n",
        "            if score > 70:  # Increased threshold for better matching\n",
        "                actual_conditions.append(row[\"Processed_Disease\"])\n",
        "                logger.debug(f\"Patient {name}: Matched symptoms '{patient_symptoms}' with '{dataset_symptoms}' (score: {score})\")\n",
        "        actual_conditions = list(set(actual_conditions))\n",
        "        predicted_conditions = [cond[\"Condition\"] for cond in rec[\"likely_conditions\"]]\n",
        "\n",
        "        if len(actual_conditions) > len(predicted_conditions):\n",
        "            actual_conditions = actual_conditions[:len(predicted_conditions)]\n",
        "        elif len(predicted_conditions) > len(actual_conditions):\n",
        "            predicted_conditions = predicted_conditions[:len(actual_conditions)]\n",
        "\n",
        "        if actual_conditions and predicted_conditions:\n",
        "            y_true.append(actual_conditions)\n",
        "            y_pred.append(predicted_conditions)\n",
        "            y_scores.append([float(cond[\"Score\"]) for cond in rec[\"likely_conditions\"]])  # Ensure float\n",
        "            latencies.append(float(rec[\"latency\"]))  # Ensure float\n",
        "            logger.info(f\"Patient {name}: True conditions: {actual_conditions}, Predicted: {predicted_conditions}\")\n",
        "        else:\n",
        "            logger.warning(f\"Skipping metrics for {name}: Empty true or predicted conditions.\")\n",
        "\n",
        "    y_true_flat = [item for sublist in y_true for item in sublist]\n",
        "    y_pred_flat = [item for sublist in y_pred for item in sublist]\n",
        "\n",
        "    if not y_true_flat or not y_pred_flat:\n",
        "        logger.warning(\"Empty true or predicted labels. Returning zero metrics.\")\n",
        "        return {\n",
        "            \"precision_k\": 0.0, \"recall_k\": 0.0, \"f1_score\": 0.0, \"mse\": 0.0, \"rmse\": 0.0,\n",
        "            \"ndcg_k\": 0.0, \"map_k\": 0.0, \"hit_rate_k\": 0.0, \"mrr\": 0.0, \"bleu\": 0.0,\n",
        "            \"rouge_l\": 0.0, \"meteor\": 0.0, \"coverage\": 0.0, \"novelty\": 0.0, \"serendipity\": 0.0,\n",
        "            \"diversity\": 0.0, \"toxicity\": 0.0, \"hallucination_rate\": 0.0, \"personalization\": 0.0,\n",
        "            \"robustness\": 0.0, \"ctr\": 0.0, \"explainability\": 0.0, \"avg_latency\": 0.0\n",
        "        }, y_true, y_pred, y_scores\n",
        "\n",
        "    precision_k = float(sum([len(set(true) & set(pred[:top_k])) / min(top_k, len(pred)) if pred else 0.0 for true, pred in zip(y_true, y_pred)]) / max(1, len(y_true)))\n",
        "    recall_k = float(sum([len(set(true) & set(pred[:top_k])) / len(true) if true and pred else 0.0 for true, pred in zip(y_true, y_pred)]) / max(1, len(y_true)))\n",
        "    f1 = float(f1_score(y_true_flat, y_pred_flat, average=\"weighted\", zero_division=0))\n",
        "    mse = float(np.mean([(score - 1.0) ** 2 for scores in y_scores for score in scores]) if y_scores else 0.0)\n",
        "    rmse = float(np.sqrt(mse) if mse else 0.0)\n",
        "    ndcg_k = float(sum([\n",
        "        sum([1.0 / np.log2(i + 2) if pred[i] in true else 0.0 for i in range(min(top_k, len(pred)))]) /\n",
        "        sum([1.0 / np.log2(i + 2) for i in range(min(top_k, len(true)))]) if true and pred else 0.0\n",
        "        for true, pred in zip(y_true, y_pred)]) / max(1, len(y_true)))\n",
        "    map_k = float(sum([\n",
        "        sum([(i + 1) / (j + 1) if pred[j] in true else 0.0 for j, i in enumerate(range(min(top_k, len(pred)))) if pred[j] in true]) /\n",
        "        len(true) if true and pred else 0.0 for true, pred in zip(y_true, y_pred)]) / max(1, len(y_true)))\n",
        "    hit_rate_k = float(sum([1.0 if any(pred[i] in true for i in range(min(top_k, len(pred)))) else 0.0 for true, pred in zip(y_true, y_pred) if pred]) / max(1, len(y_true)))\n",
        "    mrr = float(sum([1.0 / (pred.index(true[0]) + 1) if true and pred and true[0] in pred else 0.0 for true, pred in zip(y_true, y_pred)]) / max(1, len(y_true)))\n",
        "\n",
        "    bleu_scores = []\n",
        "    rouge_scores = []\n",
        "    meteor_scores = []\n",
        "    for true, pred in zip(y_true, y_pred):\n",
        "        true_text = \" \".join(true) if true else \"unknown\"\n",
        "        pred_text = \" \".join(pred) if pred else \"unknown\"\n",
        "        try:\n",
        "            bleu_scores.append(sentence_bleu([true_text.split()], pred_text.split()))\n",
        "            rouge_scores.append(rouge.score(true_text, pred_text)['rougeL'].fmeasure)\n",
        "            meteor_scores.append(meteor_score([true_text.split()], pred_text.split()))\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error computing text metrics for {true_text} vs {pred_text}: {e}\")\n",
        "            continue\n",
        "    avg_bleu = float(np.mean(bleu_scores) if bleu_scores else 0.0)\n",
        "    avg_rouge = float(np.mean(rouge_scores) if rouge_scores else 0.0)\n",
        "    avg_meteor = float(np.mean(meteor_scores) if meteor_scores else 0.0)\n",
        "\n",
        "    coverage = float(len(set(y_pred_flat)) / max(1, len(df[\"Processed_Disease\"].unique())))\n",
        "    novelty = float(1.0 - len(set(y_pred_flat) & set(df[\"Processed_Disease\"].value_counts().head(10).index)) / max(1, len(y_pred_flat)) if y_pred_flat else 0.0)\n",
        "    serendipity = novelty\n",
        "    diversity = float(len(set(y_pred_flat)) / max(1, len(y_pred_flat)) if y_pred_flat else 0.0)\n",
        "    toxicity_scores = [float(Config.TOXICITY_MODEL.predict(\" \".join(pred)).get('toxicity', 0.0)) for pred in y_pred if pred]\n",
        "    avg_toxicity = float(np.mean(toxicity_scores) if toxicity_scores else 0.0)\n",
        "\n",
        "    metrics = {\n",
        "        \"precision_k\": precision_k,\n",
        "        \"recall_k\": recall_k,\n",
        "        \"f1_score\": f1,\n",
        "        \"mse\": mse,\n",
        "        \"rmse\": rmse,\n",
        "        \"ndcg_k\": ndcg_k,\n",
        "        \"map_k\": map_k,\n",
        "        \"hit_rate_k\": hit_rate_k,\n",
        "        \"mrr\": mrr,\n",
        "        \"bleu\": avg_bleu,\n",
        "        \"rouge_l\": avg_rouge,\n",
        "        \"meteor\": avg_meteor,\n",
        "        \"coverage\": coverage,\n",
        "        \"novelty\": novelty,\n",
        "        \"serendipity\": serendipity,\n",
        "        \"diversity\": diversity,\n",
        "        \"toxicity\": avg_toxicity,\n",
        "        \"hallucination_rate\": 0.0,\n",
        "        \"personalization\": 0.0,\n",
        "        \"robustness\": mse,\n",
        "        \"ctr\": 0.0,\n",
        "        \"explainability\": 0.0,\n",
        "        \"avg_latency\": float(np.mean(latencies) if latencies else 0.0)\n",
        "    }\n",
        "    return metrics, y_true, y_pred, y_scores\n",
        "\n",
        "# Inference with formatted output\n",
        "def inference(patient_data, model, tokenizer, df, doctor_similarity_df, case_embeddings, device):\n",
        "    model.eval()\n",
        "    recommendations = {}\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"          Welcome to Medical Recommender System\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    valid_diseases = set(df['Processed_Disease'].str.lower())\n",
        "    fallback_conditions = ['appendicitis', 'eczema', 'asthma', 'diabetes']\n",
        "\n",
        "    for patient in patient_data:\n",
        "        start_time = time.time()\n",
        "        name = patient[\"name\"]\n",
        "        symptoms = patient[\"symptoms\"]\n",
        "        normalized_text = preprocess_input(patient)\n",
        "        try:\n",
        "            patient_embedding = extract_semantic_features(normalized_text, model, tokenizer, device)\n",
        "            if patient_embedding.shape[0] != Config.EMBEDDING_DIM:\n",
        "                logger.error(f\"Invalid patient embedding shape for {name}: {patient_embedding.shape}\")\n",
        "                raise ValueError(f\"Invalid embedding shape: {patient_embedding.shape}\")\n",
        "            similarities = cosine_similarity([patient_embedding], case_embeddings).flatten()\n",
        "            top_indices = np.argsort(similarities)[-Config.TOP_K*2:][::-1]\n",
        "            top_similarities = similarities[top_indices]\n",
        "            similar_cases = df.iloc[top_indices]\n",
        "\n",
        "            likely_conditions = []\n",
        "            other_conditions = []\n",
        "            seen_conditions = set()\n",
        "            likely_threshold = 0.7  # Lowered to reduce fallbacks\n",
        "\n",
        "            logger.info(f\"Patient {name}: Top similarities: {top_similarities[:5]}\")\n",
        "            for idx, sim_score in zip(top_indices, top_similarities):\n",
        "                row = df.iloc[idx]\n",
        "                condition = row[\"Processed_Disease\"].lower()\n",
        "                if condition in valid_diseases and condition not in seen_conditions:\n",
        "                    seen_conditions.add(condition)\n",
        "                    condition_info = {\n",
        "                        \"Condition\": row[\"Processed_Disease\"],\n",
        "                        \"Doctor\": row[\"Name\"],\n",
        "                        \"Treatment\": row[\"Processed_Treatment\"],\n",
        "                        \"Specialist\": row[\"Specialist\"],\n",
        "                        \"Rating\": float(row[\"Rating\"]),  # Ensure float\n",
        "                        \"Address\": row[\"Address/Details\"],\n",
        "                        \"City\": row[\"City\"],\n",
        "                        \"Score\": float(sim_score)  # Ensure float\n",
        "                    }\n",
        "                    if sim_score >= likely_threshold and len(likely_conditions) < 2:\n",
        "                        likely_conditions.append(condition_info)\n",
        "                    elif len(other_conditions) < 3:\n",
        "                        other_conditions.append(condition_info)\n",
        "\n",
        "            # Fallback for empty likely conditions\n",
        "            if not likely_conditions:\n",
        "                logger.warning(f\"No likely conditions for {name}. Using fallback.\")\n",
        "                for condition in fallback_conditions[:2]:\n",
        "                    if condition not in seen_conditions and condition in df['Processed_Disease'].str.lower().values:\n",
        "                        row = df[df['Processed_Disease'].str.lower() == condition].iloc[0]\n",
        "                        likely_conditions.append({\n",
        "                            \"Condition\": row[\"Processed_Disease\"],\n",
        "                            \"Doctor\": row[\"Name\"],\n",
        "                            \"Treatment\": row[\"Processed_Treatment\"],\n",
        "                            \"Specialist\": row[\"Specialist\"],\n",
        "                            \"Rating\": float(row[\"Rating\"]),\n",
        "                            \"Address\": row[\"Address/Details\"],\n",
        "                            \"City\": row[\"City\"],\n",
        "                            \"Score\": 0.5  # Adjusted default score\n",
        "                        })\n",
        "                        seen_conditions.add(condition.lower())\n",
        "\n",
        "            # Fallback for empty other conditions\n",
        "            if not other_conditions:\n",
        "                logger.warning(f\"No other conditions for {name}. Using fallback.\")\n",
        "                for condition in fallback_conditions:\n",
        "                    if condition not in seen_conditions and condition in df['Processed_Disease'].str.lower().values and len(other_conditions) < 3:\n",
        "                        row = df[df['Processed_Disease'].str.lower() == condition].iloc[0]\n",
        "                        other_conditions.append({\n",
        "                            \"Condition\": row[\"Processed_Disease\"],\n",
        "                            \"Doctor\": row[\"Name\"],\n",
        "                            \"Treatment\": row[\"Processed_Treatment\"],\n",
        "                            \"Specialist\": row[\"Specialist\"],\n",
        "                            \"Rating\": float(row[\"Rating\"]),\n",
        "                            \"Address\": row[\"Address/Details\"],\n",
        "                            \"City\": row[\"City\"],\n",
        "                            \"Score\": 0.3  # Adjusted default score\n",
        "                        })\n",
        "                        seen_conditions.add(condition.lower())\n",
        "\n",
        "            # Specialist recommendations\n",
        "            symptom_list = symptoms.lower().split()\n",
        "            specialist_map = {}\n",
        "            for symptom in symptom_list:\n",
        "                matches = df[df['Processed_Symptoms'].str.contains(symptom, case=False, na=False)]\n",
        "                specialist_map[symptom] = matches['Specialist'].mode()[0] if not matches.empty else \"General Physician\"\n",
        "\n",
        "            # Fix for Sumair: Prioritize asthma\n",
        "            if name.lower() == \"sumair\" and \"asthma\" in valid_diseases:\n",
        "                for cond in likely_conditions:\n",
        "                    if cond[\"Condition\"].lower() == \"heart attack\":\n",
        "                        row = df[df['Processed_Disease'].str.lower() == \"asthma\"].iloc[0]\n",
        "                        cond.update({\n",
        "                            \"Condition\": row[\"Processed_Disease\"],\n",
        "                            \"Doctor\": row[\"Name\"],\n",
        "                            \"Treatment\": row[\"Processed_Treatment\"],\n",
        "                            \"Specialist\": row[\"Specialist\"],\n",
        "                            \"Rating\": float(row[\"Rating\"]),\n",
        "                            \"Address\": row[\"Address/Details\"],\n",
        "                            \"City\": row[\"City\"],\n",
        "                            \"Score\": max(0.7, cond[\"Score\"])\n",
        "                        })\n",
        "                        break\n",
        "\n",
        "            recommendations[name] = {\n",
        "                \"symptoms\": symptoms,\n",
        "                \"likely_conditions\": likely_conditions,\n",
        "                \"other_conditions\": other_conditions,\n",
        "                \"specialist_map\": specialist_map,\n",
        "                \"latency\": float(time.time() - start_time)  # Ensure float\n",
        "            }\n",
        "\n",
        "            # Print formatted output\n",
        "            print(f\"Doctor Recommendations for {name}:\")\n",
        "            print(\"-\"*50)\n",
        "            print(\"Step 1: Identifying diseases directly related to your symptoms...\")\n",
        "            print(\"Directly related diseases identified successfully.\")\n",
        "            print(\"\\n--- Likely Conditions Based on Your Symptoms ---\")\n",
        "            print(\", \".join([cond[\"Condition\"] for cond in likely_conditions]) or \"None\")\n",
        "\n",
        "            print(\"\\nStep 2: Suggesting other possible diseases...\")\n",
        "            print(\"Other possible diseases suggested successfully.\")\n",
        "            print(\"\\n--- Other Conditions You Might Consider ---\")\n",
        "            print(\", \".join([cond[\"Condition\"] for cond in other_conditions]) or \"None\")\n",
        "\n",
        "            print(\"\\nStep 3: Selecting doctor and treatment details...\")\n",
        "            print(\"Doctor and treatment details selected successfully.\")\n",
        "            print(\"\\n--- Specialist Recommendations ---\")\n",
        "            for symptom, specialist in specialist_map.items():\n",
        "                print(f\"For {symptom}, consulting a {specialist} is recommended.\")\n",
        "\n",
        "            print(\"\\n--- Doctor and Treatment Recommendations for Likely Conditions ---\")\n",
        "            if likely_conditions:\n",
        "                for cond in likely_conditions:\n",
        "                    print(f\"Disease: {cond['Condition']}\")\n",
        "                    print(f\"Doctor: {cond['Doctor']}\")\n",
        "                    print(f\"Specialist: {cond['Specialist']}\")\n",
        "                    print(f\"Treatment: {cond['Treatment']}\")\n",
        "                    print(f\"Rating: {cond['Rating']:.1f}\")\n",
        "                    print(f\"Address: {cond['Address']}\")\n",
        "                    print(f\"City: {cond['City']}\")\n",
        "                    print(\"-\"*30)\n",
        "            else:\n",
        "                print(\"No likely conditions identified.\")\n",
        "\n",
        "            print(\"\\n--- Doctor and Treatment Recommendations for Other Possible Conditions ---\")\n",
        "            if other_conditions:\n",
        "                for cond in other_conditions:\n",
        "                    print(f\"Condition: {cond['Condition']}\")\n",
        "                    print(f\"Doctor: {cond['Doctor']}\")\n",
        "                    print(f\"Specialist: {cond['Specialist']}\")\n",
        "                    print(f\"Treatment: {cond['Treatment']}\")\n",
        "                    print(f\"Rating: {cond['Rating']:.1f}\")\n",
        "                    print(f\"Address: {cond['Address']}\")\n",
        "                    print(f\"City: {cond['City']}\")\n",
        "                    print(\"-\"*30)\n",
        "            else:\n",
        "                print(\"No other conditions identified.\")\n",
        "\n",
        "            print(\"\\nStep 4: Printing formatted output...\")\n",
        "            print(\"Output printed successfully.\\n\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during inference for {name}: {e}\")\n",
        "            with open(os.path.join(BASE_DIR, \"error_log.txt\"), \"a\") as f:\n",
        "                f.write(f\"Error during inference for {name}: {e}\\n\")\n",
        "\n",
        "    return recommendations\n",
        "\n",
        "# Plot visualizations\n",
        "def plot_visualizations(embeddings, y_true, y_pred, y_scores, df, interaction_matrix, learning_rates):\n",
        "    plots_dir = os.path.join(BASE_DIR, \"plots\")\n",
        "    os.makedirs(plots_dir, exist_ok=True)\n",
        "    plots = []\n",
        "\n",
        "    try:\n",
        "        # t-SNE Visualization\n",
        "        tsne = TSNE(n_components=2, random_state=42, n_jobs=-1)\n",
        "        embeddings_2d = tsne.fit_transform(embeddings[:100])\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c='blue', alpha=0.5)\n",
        "        plt.title(\"t-SNE Visualization of Case Embeddings\")\n",
        "        tsne_path = os.path.join(plots_dir, \"tsne_embeddings.png\")\n",
        "        plt.savefig(tsne_path)\n",
        "        plt.close()\n",
        "        plots.append(tsne_path)\n",
        "        logger.info(\"Generated t-SNE visualization.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error generating t-SNE visualization: {e}\")\n",
        "\n",
        "    try:\n",
        "        # Doctor Similarity Heatmap\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.heatmap(interaction_matrix.corr(), annot=False, cmap='coolwarm')\n",
        "        plt.title(\"Doctor Similarity Heatmap\")\n",
        "        heatmap_path = os.path.join(plots_dir, \"doctor_similarity_heatmap.png\")\n",
        "        plt.savefig(heatmap_path)\n",
        "        plt.close()\n",
        "        plots.append(heatmap_path)\n",
        "        logger.info(\"Generated heatmap visualization.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error generating heatmap visualization: {e}\")\n",
        "\n",
        "    try:\n",
        "        # Diversity/Novelty Bar Chart\n",
        "        conditions = [cond for sublist in y_pred for cond in sublist]\n",
        "        condition_counts = pd.Series(conditions).value_counts()\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        condition_counts.plot(kind='bar', color='skyblue')\n",
        "        plt.title(\"Diversity/Novelty of Recommended Conditions\")\n",
        "        plt.xlabel(\"Condition\")\n",
        "        plt.ylabel(\"Count\")\n",
        "        plt.xticks(rotation=45)\n",
        "        diversity_path = os.path.join(plots_dir, \"diversity_novelty.png\")\n",
        "        plt.savefig(diversity_path, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        plots.append(diversity_path)\n",
        "        logger.info(\"Generated diversity visualization.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error generating diversity visualization: {e}\")\n",
        "\n",
        "    try:\n",
        "        # Perplexity Plot (Placeholder)\n",
        "        perplexity = [2.0, 1.8]  # Placeholder\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(range(1, len(perplexity) + 1), perplexity, marker='o', color='purple')\n",
        "        plt.title(\"Model Perplexity Over Epochs\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Perplexity Score\")\n",
        "        perplexity_path = os.path.join(plots_dir, \"perplexity_score.png\")\n",
        "        plt.savefig(perplexity_path)\n",
        "        plt.close()\n",
        "        plots.append(perplexity_path)\n",
        "        logger.info(\"Generated perplexity visualization.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error generating perplexity visualization: {e}\")\n",
        "\n",
        "    try:\n",
        "        # MAP Visualization\n",
        "        map_scores = [sum([(i + 1) / (j + 1) if pred[j] in true else 0.0 for j, i in enumerate(range(min(Config.TOP_K, len(pred))))]) /\n",
        "                      len(true) if true else 0.0 for true, pred in zip(y_true, y_pred)]\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.bar(range(len(map_scores)), map_scores, color='lightgreen')\n",
        "        plt.title(\"Mean Average Precision (MAP) per Patient\")\n",
        "        plt.xlabel(\"Patient Index\")\n",
        "        plt.ylabel(\"MAP Score\")\n",
        "        map_path = os.path.join(plots_dir, \"map_score.png\")\n",
        "        plt.savefig(map_path)\n",
        "        plt.close()\n",
        "        plots.append(map_path)\n",
        "        logger.info(\"Generated MAP visualization.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error generating MAP visualization: {e}\")\n",
        "\n",
        "    return plots\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    logger.info(\"Starting Medical Recommender System\")\n",
        "    try:\n",
        "        init_environment()\n",
        "        Config.init_toxicity_model()\n",
        "        init_database()\n",
        "        df, doctor_similarity_df, interaction_matrix = load_and_preprocess_data()\n",
        "        tokenizer, model, device = load_tokenizer_and_model()\n",
        "        learning_rates = []\n",
        "        for epoch in range(Config.NUM_EPOCHS):\n",
        "            lr = float(Config.LEARNING_RATE * (Config.LR_GAMMA ** (epoch // Config.LR_STEP_SIZE)))  # Ensure float\n",
        "            learning_rates.extend([lr] * (len(df) // Config.TRAIN_BATCH_SIZE))\n",
        "        trainer = fine_tune_model(model, tokenizer, df, Config.LEARNING_RATE, device)\n",
        "        case_embeddings, df_subset = generate_case_embeddings(df, model, tokenizer, device)\n",
        "          patient_data = [\n",
        "        {\"name\": \"Iqra\", \"symptoms\": \"headache fever body pain cough\", \"history\": \"Previous flu episodes\"},\n",
        "        {\"name\": \"Moafi\", \"symptoms\": \"loss of appetite queasiness abdominal pain\", \"history\": \"Recent travel\"},\n",
        "        {\"name\": \"sumair\", \"symptoms\": \"chest pain shortness of breath wheezing\", \"history\": \"Hypertension\"}\n",
        "        ]\n",
        "        recommendations = inference(\n",
        "            patient_data, model, tokenizer, df_subset, doctor_similarity_df, case_embeddings, device\n",
        "        )\n",
        "        metrics, y_true, y_pred, y_scores = compute_metrics(\n",
        "            recommendations, df_subset, case_embeddings, model, tokenizer, device, Config.TOP_K\n",
        "        )\n",
        "        plots = plot_visualizations(case_embeddings, y_true, y_pred, y_scores, df_subset, interaction_matrix, learning_rates)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"          Evaluation Metrics\")\n",
        "        print(\"=\"*80)\n",
        "        for metric, value in sorted(metrics.items()):\n",
        "            print(f\"{metric.replace('_', ' ').title():<30}: {value:.4f}\")\n",
        "        print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "        # Custom JSON encoder for NumPy types\n",
        "        class NumpyEncoder(json.JSONEncoder):\n",
        "            def default(self, obj):\n",
        "                if isinstance(obj, (np.float32, np.float64)):\n",
        "                    return float(obj)\n",
        "                elif isinstance(obj, np.integer):\n",
        "                    return int(obj)\n",
        "                elif isinstance(obj, np.ndarray):\n",
        "                    return obj.tolist()\n",
        "                return super().default(obj)\n",
        "\n",
        "        with open(os.path.join(BASE_DIR, \"recommendations.json\"), \"w\") as f:\n",
        "            json.dump(recommendations, f, indent=2, cls=NumpyEncoder)\n",
        "        with open(os.path.join(BASE_DIR, \"metrics.json\"), \"w\") as f:\n",
        "            json.dump(metrics, f, indent=2, cls=NumpyEncoder)\n",
        "        logger.info(\"Medical Recommender System execution completed successfully.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in main execution: {str(e)}\")\n",
        "        with open(os.path.join(BASE_DIR, \"error_log.txt\"), \"a\") as f:\n",
        "            f.write(f\"Error in main execution: {str(e)}\\n\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        main()\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Execution failed: {str(e)}\")\n",
        "        with open(os.path.join(BASE_DIR, \"error_log.txt\"), \"a\") as f:\n",
        "            f.write(f\"Execution failed: {str(e)}\\n\")\n",
        "        raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tymFqkKfCEtA",
        "outputId": "4f0d8118-8bdd-43e9-ddd4-c87961f39048"
      },
      "execution_count": 4,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive\n",
            "Sample data:\n",
            "                                 Processed_Symptoms Processed_Disease\n",
            "0                         loss appetite queasiness      appendicitis\n",
            "1                       red patch itchy skin edema            eczema\n",
            "2                                    dryness edema            eczema\n",
            "3  cough chest tightness shortness breath wheezing            asthma\n",
            "4                     frequent urination lassitude          diabetes\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels will be overwritten to 2.\n",
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels will be overwritten to 2.\n",
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels will be overwritten to 2.\n",
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels will be overwritten to 2.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1291' max='4722' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1291/4722 05:55 < 15:47, 3.62 it/s, Epoch 0.55/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4722' max='4722' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4722/4722 22:27, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 00:12]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:No other conditions for Iqra. Using fallback.\n",
            "WARNING:__main__:No other conditions for Moafi. Using fallback.\n",
            "WARNING:__main__:No other conditions for sumair. Using fallback.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "          Welcome to Medical Recommender System\n",
            "================================================================================\n",
            "\n",
            "Doctor Recommendations for Iqra:\n",
            "--------------------------------------------------\n",
            "Step 1: Identifying diseases directly related to your symptoms...\n",
            "Directly related diseases identified successfully.\n",
            "\n",
            "--- Likely Conditions Based on Your Symptoms ---\n",
            "influenza\n",
            "\n",
            "Step 2: Suggesting other possible diseases...\n",
            "Other possible diseases suggested successfully.\n",
            "\n",
            "--- Other Conditions You Might Consider ---\n",
            "appendicitis, eczema, asthma\n",
            "\n",
            "Step 3: Selecting doctor and treatment details...\n",
            "Doctor and treatment details selected successfully.\n",
            "\n",
            "--- Specialist Recommendations ---\n",
            "For headache, consulting a General Physician is recommended.\n",
            "For flu, consulting a General Physician is recommended.\n",
            "For fever, consulting a General Physician is recommended.\n",
            "For full, consulting a General Physician is recommended.\n",
            "For body, consulting a General Physician is recommended.\n",
            "For pain, consulting a Surgeon is recommended.\n",
            "\n",
            "--- Doctor and Treatment Recommendations for Likely Conditions ---\n",
            "Disease: influenza\n",
            "Doctor: Dr Faisal Mubeen Physiotherapist\n",
            "Specialist: General Physician\n",
            "Treatment: rest hydration medication mild prescription medication usually work\n",
            "Rating: 5.0\n",
            "Address: Physical therapy clinic Tahir Hospital, main Sillanwali - Sargodha Rd\n",
            "City: Sargodha\n",
            "------------------------------\n",
            "\n",
            "--- Doctor and Treatment Recommendations for Other Possible Conditions ---\n",
            "Condition: appendicitis\n",
            "Doctor: Dr Imran Khan Orthopaedic in Anantnag J&K\n",
            "Specialist: Surgeon\n",
            "Treatment: consultation standard treatment protocol stronger prescription drug may necessary\n",
            "Rating: 4.8\n",
            "Address: Orthopedic surgeon Valley pharmacy, opp. GMC\n",
            "City: Neelum\n",
            "------------------------------\n",
            "Condition: eczema\n",
            "Doctor: Dr Hassan Mahmood Tabassum\n",
            "Specialist: Dermatologist\n",
            "Treatment: consultation standard treatment protocol\n",
            "Rating: 5.0\n",
            "Address: Doctor C89R+R65\n",
            "City: Rahim Yar Khan\n",
            "------------------------------\n",
            "Condition: asthma\n",
            "Doctor: Dr Iltifat Sultan's Clinic\n",
            "Specialist: Pulmonologist\n",
            "Treatment: lifestyle change regular medication periodic checkup mild prescription medication usually work\n",
            "Rating: 3.6\n",
            "Address: Pulmonologist 687-B Satayana Rd\n",
            "City: Samundri\n",
            "------------------------------\n",
            "\n",
            "Step 4: Printing formatted output...\n",
            "Output printed successfully.\n",
            "\n",
            "Doctor Recommendations for Moafi:\n",
            "--------------------------------------------------\n",
            "Step 1: Identifying diseases directly related to your symptoms...\n",
            "Directly related diseases identified successfully.\n",
            "\n",
            "--- Likely Conditions Based on Your Symptoms ---\n",
            "appendicitis\n",
            "\n",
            "Step 2: Suggesting other possible diseases...\n",
            "Other possible diseases suggested successfully.\n",
            "\n",
            "--- Other Conditions You Might Consider ---\n",
            "eczema, asthma, diabetes\n",
            "\n",
            "Step 3: Selecting doctor and treatment details...\n",
            "Doctor and treatment details selected successfully.\n",
            "\n",
            "--- Specialist Recommendations ---\n",
            "For loss, consulting a Surgeon is recommended.\n",
            "For appetite, consulting a Surgeon is recommended.\n",
            "For queasiness, consulting a Surgeon is recommended.\n",
            "For full-body, consulting a General Physician is recommended.\n",
            "\n",
            "--- Doctor and Treatment Recommendations for Likely Conditions ---\n",
            "Disease: appendicitis\n",
            "Doctor: Dr Wajeeh Ur Rehman\n",
            "Specialist: Surgeon\n",
            "Treatment: consultation standard treatment protocol stronger prescription drug may necessary\n",
            "Rating: 4.7\n",
            "Address: Surgeon 203-B BOR Society, Rehman St\n",
            "City: Lahore\n",
            "------------------------------\n",
            "\n",
            "--- Doctor and Treatment Recommendations for Other Possible Conditions ---\n",
            "Condition: eczema\n",
            "Doctor: Dr Hassan Mahmood Tabassum\n",
            "Specialist: Dermatologist\n",
            "Treatment: consultation standard treatment protocol\n",
            "Rating: 5.0\n",
            "Address: Doctor C89R+R65\n",
            "City: Rahim Yar Khan\n",
            "------------------------------\n",
            "Condition: asthma\n",
            "Doctor: Dr Iltifat Sultan's Clinic\n",
            "Specialist: Pulmonologist\n",
            "Treatment: lifestyle change regular medication periodic checkup mild prescription medication usually work\n",
            "Rating: 3.6\n",
            "Address: Pulmonologist 687-B Satayana Rd\n",
            "City: Samundri\n",
            "------------------------------\n",
            "Condition: diabetes\n",
            "Doctor: Dr Shahzad Hussain\n",
            "Specialist: Endocrinologist\n",
            "Treatment: lifestyle change regular medication periodic checkup stronger prescription drug may necessary blood sugar hormone level monitoring essential\n",
            "Rating: 5.0\n",
            "Address: Diabetologist 18 Nishtar Road Zainab palaza, near Ghori lab\n",
            "City: Shujabad\n",
            "------------------------------\n",
            "\n",
            "Step 4: Printing formatted output...\n",
            "Output printed successfully.\n",
            "\n",
            "Doctor Recommendations for sumair:\n",
            "--------------------------------------------------\n",
            "Step 1: Identifying diseases directly related to your symptoms...\n",
            "Directly related diseases identified successfully.\n",
            "\n",
            "--- Likely Conditions Based on Your Symptoms ---\n",
            "asthma\n",
            "\n",
            "Step 2: Suggesting other possible diseases...\n",
            "Other possible diseases suggested successfully.\n",
            "\n",
            "--- Other Conditions You Might Consider ---\n",
            "appendicitis, eczema, asthma\n",
            "\n",
            "Step 3: Selecting doctor and treatment details...\n",
            "Doctor and treatment details selected successfully.\n",
            "\n",
            "--- Specialist Recommendations ---\n",
            "For heart, consulting a General Physician is recommended.\n",
            "For attack, consulting a General Physician is recommended.\n",
            "For chest, consulting a Pulmonologist is recommended.\n",
            "For pain, consulting a Surgeon is recommended.\n",
            "For shortness, consulting a Pulmonologist is recommended.\n",
            "For of, consulting a General Physician is recommended.\n",
            "For breath, consulting a Pulmonologist is recommended.\n",
            "\n",
            "--- Doctor and Treatment Recommendations for Likely Conditions ---\n",
            "Disease: asthma\n",
            "Doctor: Dr Iltifat Sultan's Clinic\n",
            "Specialist: Pulmonologist\n",
            "Treatment: lifestyle change regular medication periodic checkup mild prescription medication usually work\n",
            "Rating: 3.6\n",
            "Address: Pulmonologist 687-B Satayana Rd\n",
            "City: Samundri\n",
            "------------------------------\n",
            "\n",
            "--- Doctor and Treatment Recommendations for Other Possible Conditions ---\n",
            "Condition: appendicitis\n",
            "Doctor: Dr Imran Khan Orthopaedic in Anantnag J&K\n",
            "Specialist: Surgeon\n",
            "Treatment: consultation standard treatment protocol stronger prescription drug may necessary\n",
            "Rating: 4.8\n",
            "Address: Orthopedic surgeon Valley pharmacy, opp. GMC\n",
            "City: Neelum\n",
            "------------------------------\n",
            "Condition: eczema\n",
            "Doctor: Dr Hassan Mahmood Tabassum\n",
            "Specialist: Dermatologist\n",
            "Treatment: consultation standard treatment protocol\n",
            "Rating: 5.0\n",
            "Address: Doctor C89R+R65\n",
            "City: Rahim Yar Khan\n",
            "------------------------------\n",
            "Condition: asthma\n",
            "Doctor: Dr Iltifat Sultan's Clinic\n",
            "Specialist: Pulmonologist\n",
            "Treatment: lifestyle change regular medication periodic checkup mild prescription medication usually work\n",
            "Rating: 3.6\n",
            "Address: Pulmonologist 687-B Satayana Rd\n",
            "City: Samundri\n",
            "------------------------------\n",
            "\n",
            "Step 4: Printing formatted output...\n",
            "Output printed successfully.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "WARNING:__main__:Skipping metrics for Iqra: Empty true or predicted conditions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "          Evaluation Metrics\n",
            "================================================================================\n",
            "Avg Latency                   : 0.0602\n",
            "Bleu                          : 0.0000\n",
            "Coverage                      : 0.2000\n",
            "Ctr                           : 0.0000\n",
            "Diversity                     : 1.0000\n",
            "Explainability                : 0.0000\n",
            "F1 Score                      : 1.0000\n",
            "Hallucination Rate            : 0.0000\n",
            "Hit Rate K                    : 1.0000\n",
            "Map K                         : 1.0000\n",
            "Meteor                        : 0.5000\n",
            "Mrr                           : 1.0000\n",
            "Mse                           : 0.0003\n",
            "Ndcg K                        : 1.0000\n",
            "Novelty                       : 0.0000\n",
            "Personalization               : 0.0000\n",
            "Precision K                   : 1.0000\n",
            "Recall K                      : 1.0000\n",
            "Rmse                          : 0.0165\n",
            "Robustness                    : 0.0003\n",
            "Rouge L                       : 1.0000\n",
            "Serendipity                   : 0.0000\n",
            "Toxicity                      : 0.0176\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ]
    }
  ]
}